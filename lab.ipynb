{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to your workspace\n",
        "\n",
        "The code below uses the configuration file to connect to your workspace. The first time you run it in a notebook session, you'll be prompted to sign into Azure by clicking the https://microsoft.com/devicelogin link, entering an automatically generated code, and signing into Azure. After you have successfully signed in, you can close the browser tab that was opened and return to this notebook."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "\n",
        "\n",
        "# Load the workspace from the saved config file\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1611273514836
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge 1: Train and Register Model\n",
        "\n",
        "Before we deploy a real-time endpoint there should be a model to be deployed. The below code trains and registers that model. Run the following cell to complete this challenge. Please be adviced that this will take some time as some configurations and training needs to be completed."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from azureml.core import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "from azureml.core.conda_dependencies import CondaDependencies \n",
        "from azureml.train.sklearn import SKLearn\n",
        "from azureml.core import Environment, Experiment\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "\n",
        "#######################################\n",
        "# Working with data\n",
        "#######################################\n",
        "print(f\"{'#'*50}\\n# Working with data \\n{'#'*50}\")\n",
        "# Get the default datastore\n",
        "default_ds = ws.get_default_datastore()\n",
        "\n",
        "default_ds.upload_files(files=['./data/flight_delays_data.csv'], # Upload the flight_delays csv files in /data\n",
        "                       target_path='data/', # Put it in a folder path in the datastore\n",
        "                       overwrite=True, # Replace existing files of the same name\n",
        "                       show_progress=True)\n",
        "\n",
        "if 'flight_delays_data' not in ws.datasets:\n",
        "    #Create a tabular dataset from the path on the datastore (this may take a short while)\n",
        "    csv_path = [(default_ds, 'data/flight_delays_data.csv')]\n",
        "    tab_data_set = Dataset.Tabular.from_delimited_files(path=csv_path)\n",
        "\n",
        "    # Register the tabular dataset\n",
        "    try:\n",
        "        tab_data_set = tab_data_set.register(workspace=ws, \n",
        "                                name='flight_delays_data',\n",
        "                                description='flight delays data',\n",
        "                                tags = {'format':'CSV'},\n",
        "                                create_new_version=True)\n",
        "        print('Dataset registered.')\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "else:\n",
        "    print('Dataset already registered.')\n",
        "\n",
        "\n",
        "###################################\n",
        "# Feature Engineering\n",
        "###################################\n",
        "print(f\"\\n\\n{'#'*50}\\n# Feature Engineering \\n{'#'*50}\")\n",
        "# Get the training dataset\n",
        "dataset = ws.datasets.get('flight_delays_data')\n",
        "dataset = dataset.to_pandas_dataframe().dropna()\n",
        "\n",
        "# Remove target leaker and features that are not useful\n",
        "target_leakers = ['DepDel15','ArrDelay','Cancelled','Year']\n",
        "dataset.drop(columns=target_leakers, axis=1, inplace=True)\n",
        "\n",
        "# convert some columns to categorical features\n",
        "columns_as_categorical = ['OriginAirportID','DestAirportID','ArrDel15']\n",
        "dataset[columns_as_categorical] = dataset[columns_as_categorical].astype('object')\n",
        "\n",
        "# The labelEncoder and OneHotEncoder only works on categorical features. We need first to extract the categorial featuers using boolean mask.\n",
        "categorical_feature_mask = dataset.dtypes == object \n",
        "categorical_cols = dataset.columns[categorical_feature_mask].tolist()\n",
        "categorical_cols\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Apply LabelEncoder on each of the categorical columns:\n",
        "dataset[categorical_cols] = dataset[categorical_cols].apply(lambda col:le.fit_transform(col))\n",
        "\n",
        "# Drop all null values\n",
        "dataset.dropna(inplace=True)\n",
        "\n",
        "train_ds, test_ds = dataset.loc[dataset['Month'] < 9], dataset.loc[dataset['Month'] >= 9]\n",
        "train_count = train_ds.Month.count()\n",
        "test_count = test_ds.Month.count()\n",
        "print('Test data ratio:',(test_count/(test_count+train_count))*100)\n",
        "\n",
        "\n",
        "#########################################\n",
        "# Environment setup\n",
        "#########################################\n",
        "print(f\"\\n\\n{'#'*50}\\n# Environment setup \\n{'#'*50}\")\n",
        "# Create a Python environment for the experiment\n",
        "flight_delays_env = Environment(\"flight-delays-experiment-env\")\n",
        "flight_delays_env.python.user_managed_dependencies = False # Let Azure ML manage dependencies\n",
        "flight_delays_env.docker.enabled = True # Use a docker container\n",
        "\n",
        "# Create a set of package dependencies (conda or pip as required)\n",
        "flight_delays_packages = CondaDependencies.create(conda_packages=['scikit-learn'],\n",
        "                                          pip_packages=['azureml-defaults', 'azureml-dataprep[pandas]', 'matplotlib', 'seaborn'])\n",
        "\n",
        "# Add the dependencies to the environment\n",
        "flight_delays_env.python.conda_dependencies = flight_delays_packages\n",
        "\n",
        "print(flight_delays_env.name, 'defined.')\n",
        "\n",
        "# Register the environment\n",
        "flight_delays_env.register(workspace=ws)\n",
        "print(flight_delays_env.name, 'registered.')\n",
        "\n",
        "\n",
        "\n",
        "##########################################\n",
        "# Create remote training cluster\n",
        "##########################################\n",
        "print(f\"\\n\\n{'#'*50}\\n# Create remote training cluster \\n{'#'*50}\")\n",
        "\n",
        "cluster_name = \"aml-cluster\"\n",
        "try:\n",
        "    # Get the cluster if it exists\n",
        "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    # If not, create it\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS2_V2', max_nodes=2)\n",
        "    training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "\n",
        "training_cluster.wait_for_completion(show_output=True)\n",
        "\n",
        "\n",
        "\n",
        "############################################################\n",
        "# Train Model\n",
        "############################################################\n",
        "print(f\"\\n\\n{'#'*50}\\n# Training Model \\n{'#'*50}\")\n",
        "# Get the environment\n",
        "registered_env = Environment.get(ws, 'flight-delays-experiment-env')\n",
        "\n",
        "# specify cluster name\n",
        "cluster_name = \"aml-cluster\"\n",
        "\n",
        "# Set the script parameters\n",
        "script_params = {\n",
        "    '--regularization': 0.1\n",
        "}\n",
        "experiment_folder = 'flight_delays'\n",
        "\n",
        "# Get the training dataset\n",
        "flight_delays_ds = ws.datasets.get(\"flight_delays_data\")\n",
        "\n",
        "# Create an estimator\n",
        "estimator = SKLearn(source_directory=experiment_folder,\n",
        "                      inputs=[flight_delays_ds.as_named_input('flight_delays_data')],\n",
        "                      script_params=script_params,\n",
        "                      compute_target = cluster_name, # Run the experiment on the remote compute target\n",
        "                      environment_definition = registered_env,\n",
        "                      entry_script='flight_delays_training.py')\n",
        "\n",
        "# Create an experiment\n",
        "experiment = Experiment(workspace = ws, name = 'flight-delays-training')\n",
        "\n",
        "# Run the experiment\n",
        "run = experiment.submit(config=estimator)\n",
        "# Show the run details while running\n",
        "RunDetails(run).show()\n",
        "run.wait_for_completion()\n",
        "\n",
        "\n",
        "#########################################################\n",
        "# Register the model\n",
        "#########################################################\n",
        "print(f\"\\n\\n{'#'*50}\\n# Registering the model \\n{'#'*50}\")\n",
        "run.register_model(model_path='outputs/flight_delays_model.pkl', model_name='flight_delays_model',\n",
        "                   tags={'Training context':'Parameterized SKLearn Estimator'},\n",
        "                   properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\n",
        "print(f\"\\n\\n{'#'*50}\\n# Model registered. \\n{'#'*50}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611271565185
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge 2: Create an Entry Script and Execution Environment\n",
        "We're going to create a web service to host this model, and this will require some code and configuration files; so let's create a folder for those."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_name = 'flight_delays_service'\n",
        "\n",
        "# Create a folder for the web service files\n",
        "experiment_folder = './' + folder_name\n",
        "os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "print(folder_name, 'folder created.')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611273067460
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The web service where we deploy the model will need some Python code to load the input data, get the model from the workspace, and generate and return predictions. We'll save this code in an *entry script* that will be deployed to the web service:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The web service will be hosted in a container, and the container will need to install any required Python dependencies when it gets initialized. In this case, our scoring code requires **scikit-learn, matplotlib and seaborn**, so we'll create a .yml file that tells the container host to install this into the environment."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611273343700
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge 3: Deploy model to a Web Service hosted on Azure Container Instance (ACI)\n",
        "When you want to test a model deployment, or if your deployment is very low-scale and CPU-based, Azure Container Instances (ACI) is a good option. This fully managed service is the fastest and most straightforward way to deploy an isolated container in Azure, which means that no cluster management or orchestration is required.\n",
        "\n",
        "Unlike deploying to AKS, you do not need to create ACI containers in advance because they are created on the fly. This means you can go straight to deploying to ACI."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611274705406
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge 4: Consume the Real-time Endpoint\n",
        "\n",
        "Let's determine the URL to which these applications must submit their requests as well as the keys:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611274705559
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you know the endpoint URI, an application can simply make an HTTP request, sending the flight data in JSON (or binary) format, and receive back the predicted class(es)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "x_new = [[4, 19, 5, 4, 18, 36, 837, -3.0, 1138]]\n",
        "\n",
        "# Convert the array to a serializable list in a JSON document\n",
        "input_json = json.dumps({\"data\": x_new})\n",
        "\n",
        "# Set the content type\n",
        "request_headers = {\"Content-Type\": \"application/json\",\n",
        "                   \"Authorization\": \"Bearer \" + primary_key}\n",
        "\n",
        "predictions = requests.post(endpoint, input_json, headers=request_headers)\n",
        "\n",
        "predicted_classes = json.loads(predictions.json())\n",
        "\n",
        "for i in range(len(x_new)):\n",
        "    print(f\"Flight {x_new[i]} -> {predicted_classes[i]}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611274751500
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.7 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
        }
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.8.7-final",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}