{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to your workspace\n",
        "\n",
        "The code below uses the configuration file to connect to your workspace. The first time you run it in a notebook session, you'll be prompted to sign into Azure by clicking the https://microsoft.com/devicelogin link, entering an automatically generated code, and signing into Azure. After you have successfully signed in, you can close the browser tab that was opened and return to this notebook."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "\n",
        "\n",
        "# Load the workspace from the saved config file\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1611273514836
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge 1: Train and Register Model\n",
        "\n",
        "Before we deploy a real-time endpoint there should be a model to be deployed. The below code trains and registers that model. Run the following cell to complete this challenge. Please be adviced that this will take some time as some configurations and training needs to be completed."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from azureml.core import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "from azureml.core.conda_dependencies import CondaDependencies \n",
        "from azureml.train.sklearn import SKLearn\n",
        "from azureml.core import Environment, Experiment\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "\n",
        "#######################################\n",
        "# Working with data\n",
        "#######################################\n",
        "print(f\"{'#'*50}\\n# Working with data \\n{'#'*50}\")\n",
        "# Get the default datastore\n",
        "default_ds = ws.get_default_datastore()\n",
        "\n",
        "default_ds.upload_files(files=['./data/flight_delays_data.csv'], # Upload the flight_delays csv files in /data\n",
        "                       target_path='data/', # Put it in a folder path in the datastore\n",
        "                       overwrite=True, # Replace existing files of the same name\n",
        "                       show_progress=True)\n",
        "\n",
        "if 'flight_delays_data' not in ws.datasets:\n",
        "    #Create a tabular dataset from the path on the datastore (this may take a short while)\n",
        "    csv_path = [(default_ds, 'data/flight_delays_data.csv')]\n",
        "    tab_data_set = Dataset.Tabular.from_delimited_files(path=csv_path)\n",
        "\n",
        "    # Register the tabular dataset\n",
        "    try:\n",
        "        tab_data_set = tab_data_set.register(workspace=ws, \n",
        "                                name='flight_delays_data',\n",
        "                                description='flight delays data',\n",
        "                                tags = {'format':'CSV'},\n",
        "                                create_new_version=True)\n",
        "        print('Dataset registered.')\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "else:\n",
        "    print('Dataset already registered.')\n",
        "\n",
        "\n",
        "###################################\n",
        "# Feature Engineering\n",
        "###################################\n",
        "print(f\"\\n\\n{'#'*50}\\n# Feature Engineering \\n{'#'*50}\")\n",
        "# Get the training dataset\n",
        "dataset = ws.datasets.get('flight_delays_data')\n",
        "dataset = dataset.to_pandas_dataframe().dropna()\n",
        "\n",
        "# Remove target leaker and features that are not useful\n",
        "target_leakers = ['DepDel15','ArrDelay','Cancelled','Year']\n",
        "dataset.drop(columns=target_leakers, axis=1, inplace=True)\n",
        "\n",
        "# convert some columns to categorical features\n",
        "columns_as_categorical = ['OriginAirportID','DestAirportID','ArrDel15']\n",
        "dataset[columns_as_categorical] = dataset[columns_as_categorical].astype('object')\n",
        "\n",
        "# The labelEncoder and OneHotEncoder only works on categorical features. We need first to extract the categorial featuers using boolean mask.\n",
        "categorical_feature_mask = dataset.dtypes == object \n",
        "categorical_cols = dataset.columns[categorical_feature_mask].tolist()\n",
        "categorical_cols\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Apply LabelEncoder on each of the categorical columns:\n",
        "dataset[categorical_cols] = dataset[categorical_cols].apply(lambda col:le.fit_transform(col))\n",
        "\n",
        "# Drop all null values\n",
        "dataset.dropna(inplace=True)\n",
        "\n",
        "train_ds, test_ds = dataset.loc[dataset['Month'] < 9], dataset.loc[dataset['Month'] >= 9]\n",
        "train_count = train_ds.Month.count()\n",
        "test_count = test_ds.Month.count()\n",
        "print('Test data ratio:',(test_count/(test_count+train_count))*100)\n",
        "\n",
        "\n",
        "#########################################\n",
        "# Environment setup\n",
        "#########################################\n",
        "print(f\"\\n\\n{'#'*50}\\n# Environment setup \\n{'#'*50}\")\n",
        "# Create a Python environment for the experiment\n",
        "flight_delays_env = Environment(\"flight-delays-experiment-env\")\n",
        "flight_delays_env.python.user_managed_dependencies = False # Let Azure ML manage dependencies\n",
        "flight_delays_env.docker.enabled = True # Use a docker container\n",
        "\n",
        "# Create a set of package dependencies (conda or pip as required)\n",
        "flight_delays_packages = CondaDependencies.create(conda_packages=['scikit-learn'],\n",
        "                                          pip_packages=['azureml-defaults', 'azureml-dataprep[pandas]', 'matplotlib', 'seaborn'])\n",
        "\n",
        "# Add the dependencies to the environment\n",
        "flight_delays_env.python.conda_dependencies = flight_delays_packages\n",
        "\n",
        "print(flight_delays_env.name, 'defined.')\n",
        "\n",
        "# Register the environment\n",
        "flight_delays_env.register(workspace=ws)\n",
        "print(flight_delays_env.name, 'registered.')\n",
        "\n",
        "\n",
        "\n",
        "##########################################\n",
        "# Create remote training cluster\n",
        "##########################################\n",
        "print(f\"\\n\\n{'#'*50}\\n# Create remote training cluster \\n{'#'*50}\")\n",
        "\n",
        "cluster_name = \"aml-cluster\"\n",
        "try:\n",
        "    # Get the cluster if it exists\n",
        "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    # If not, create it\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS2_V2', max_nodes=2)\n",
        "    training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "\n",
        "training_cluster.wait_for_completion(show_output=True)\n",
        "\n",
        "\n",
        "\n",
        "############################################################\n",
        "# Train Model\n",
        "############################################################\n",
        "print(f\"\\n\\n{'#'*50}\\n# Training Model \\n{'#'*50}\")\n",
        "# Get the environment\n",
        "registered_env = Environment.get(ws, 'flight-delays-experiment-env')\n",
        "\n",
        "# specify cluster name\n",
        "cluster_name = \"aml-cluster\"\n",
        "\n",
        "# Set the script parameters\n",
        "script_params = {\n",
        "    '--regularization': 0.1\n",
        "}\n",
        "experiment_folder = 'flight_delays'\n",
        "\n",
        "# Get the training dataset\n",
        "flight_delays_ds = ws.datasets.get(\"flight_delays_data\")\n",
        "\n",
        "# Create an estimator\n",
        "estimator = SKLearn(source_directory=experiment_folder,\n",
        "                      inputs=[flight_delays_ds.as_named_input('flight_delays_data')],\n",
        "                      script_params=script_params,\n",
        "                      compute_target = cluster_name, # Run the experiment on the remote compute target\n",
        "                      environment_definition = registered_env,\n",
        "                      entry_script='flight_delays_training.py')\n",
        "\n",
        "# Create an experiment\n",
        "experiment = Experiment(workspace = ws, name = 'flight-delays-training')\n",
        "\n",
        "# Run the experiment\n",
        "run = experiment.submit(config=estimator)\n",
        "# Show the run details while running\n",
        "RunDetails(run).show()\n",
        "run.wait_for_completion()\n",
        "\n",
        "\n",
        "#########################################################\n",
        "# Register the model\n",
        "#########################################################\n",
        "print(f\"\\n\\n{'#'*50}\\n# Registering the model \\n{'#'*50}\")\n",
        "run.register_model(model_path='outputs/flight_delays_model.pkl', model_name='flight_delays_model',\n",
        "                   tags={'Training context':'Parameterized SKLearn Estimator'},\n",
        "                   properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\n",
        "print(\"\\nModel registered.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611271565185
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge 2: Create an Entry Script and Execution Environment\n",
        "We're going to create a web service to host this model, and this will require some code and configuration files; so let's create a folder for those."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_name = 'flight_delays_service'\n",
        "\n",
        "# Create a folder for the web service files\n",
        "experiment_folder = './' + folder_name\n",
        "os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "print(folder_name, 'folder created.')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611273067460
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The web service where we deploy the model will need some Python code to load the input data, get the model from the workspace, and generate and return predictions. We'll save this code in an *entry script* that will be deployed to the web service:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $folder_name/scoring_script.py\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "from azureml.core.model import Model\n",
        "\n",
        "# Called when the service is loaded\n",
        "def init():\n",
        "    global model\n",
        "    # Get the path to the deployed model file and load it\n",
        "    model_path = Model.get_model_path('flight_delays_model')\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "\n",
        "# Called when a request is received\n",
        "def run(raw_data):\n",
        "    # Get the input data as a numpy array\n",
        "    data = np.array(json.loads(raw_data)['data'])\n",
        "    # Get a prediction from the model\n",
        "    predictions = model.predict(data)\n",
        "    # Get the corresponding classname for each prediction (0 or 1)\n",
        "    classnames = ['no-delay', 'delay']\n",
        "    predicted_classes = []\n",
        "    for prediction in predictions:\n",
        "        predicted_classes.append(classnames[prediction])\n",
        "    # Return the predictions as JSON\n",
        "    return json.dumps(predicted_classes)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The web service will be hosted in a container, and the container will need to install any required Python dependencies when it gets initialized. In this case, our scoring code requires **scikit-learn, matplotlib and seaborn**, so we'll create a .yml file that tells the container host to install this into the environment."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.conda_dependencies import CondaDependencies \n",
        "\n",
        "# Add the dependencies for our model (AzureML defaults is already included)\n",
        "myenv = CondaDependencies()\n",
        "myenv.add_conda_package(\"scikit-learn\")\n",
        "myenv.add_pip_package(\"matplotlib\")\n",
        "myenv.add_pip_package('seaborn')\n",
        "\n",
        "# Save the environment config as a .yml file\n",
        "env_file = folder_name + \"/flight_delays_env.yml\"\n",
        "with open(env_file,\"w\") as f:\n",
        "    f.write(myenv.serialize_to_string())\n",
        "print(\"Saved dependency info in\", env_file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611273343700
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge 3: Provision an Azure Kubernetes Services (AKS) Inference Cluster\n",
        "\n",
        "This is a one time setup. You can reuse this cluster for multiple deployments after it has been created. If you delete the cluster or the resource group that contains it, then you would have to recreate it.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AksCompute\n",
        "\n",
        "prov_config = AksCompute.provisioning_configuration()\n",
        "\n",
        "aks_name = 'aks-compute' \n",
        "# Create the aks-compute\n",
        "try:\n",
        "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
        "    print(\"Found existing aks-compute, use it.\")\n",
        "except ComputeTargetException:\n",
        "    aks_target = ComputeTarget.create(workspace = ws, \n",
        "                                    name = aks_name, \n",
        "                                    provisioning_configuration = prov_config)\n",
        "\n",
        "    aks_target.wait_for_completion(show_output = True)\n",
        "print(aks_target.provisioning_state)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611274054044
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge 4: Deploy model to a Web Service hosted on Azure Kubernetes Service (AKS)\n",
        "Deployment will take some time as it first runs a process to create a container image, and then runs a process to create a web service based on the image. When deployment has completed successfully, you'll see a status of **Healthy**."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.webservice import AksWebservice, Webservice\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core import Model\n",
        "from azureml.exceptions import WebserviceException\n",
        "\n",
        "model = ws.models['flight_delays_model']\n",
        "aks_service_name = \"flights-delay-aks-service\"\n",
        "\n",
        "# Configure the scoring environment\n",
        "aks_inference_config = InferenceConfig(runtime= \"python\",\n",
        "                                   source_directory = folder_name,\n",
        "                                   entry_script=\"scoring_script.py\",\n",
        "                                   conda_file=\"flight_delays_env.yml\")\n",
        "\n",
        "aks_deployment_config = AksWebservice.deploy_configuration(cpu_cores = 1, \n",
        "                                                        memory_gb = 1, \n",
        "                                                        compute_target_name='aks-compute',\n",
        "                                                        auth_enabled=True,\n",
        "                                                        autoscale_enabled=True,\n",
        "                                                        tags = {'name': aks_service_name},\n",
        "                                                        description = 'Flights delay predictor AKS Web Service'\n",
        "                                                        )\n",
        "\n",
        "try:\n",
        "    aks_service = Model.deploy(workspace=ws, \n",
        "                            name=aks_service_name, \n",
        "                            models=[model], \n",
        "                            inference_config=aks_inference_config, \n",
        "                            deployment_config=aks_deployment_config\n",
        "                            )\n",
        "    aks_service.wait_for_deployment(show_output=True)\n",
        "except WebserviceException as e: \n",
        "    print(f\"Service {aks_service_name} with the same name already exists. Use it\")\n",
        "    aks_service = Webservice(workspace=ws, name=aks_service_name)\n",
        "\n",
        "service = aks_service"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611274705406
        }
      }
    },
    {
      "source": [
        "## Challenge (optional): Deploy model to a Web Service hosted on Azure Container Instance (ACI)\n",
        "When you want to test a model deployment, or if your deployment is very low-scale and CPU-based, Azure Container Instances (ACI) is a good option. This fully managed service is the fastest and most straightforward way to deploy an isolated container in Azure, which means that no cluster management or orchestration is required.\n",
        "\n",
        "Unlike deploying to AKS, you do not need to create ACI containers in advance because they are created on the fly. This means you can go straight to deploying to ACI."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.webservice import AciWebservice, Webservice\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core import Model\n",
        "from azureml.exceptions import WebserviceException\n",
        "\n",
        "\n",
        "model = ws.models['flight_delays_model']\n",
        "aci_service_name = \"flights-delay-aci-service\"\n",
        "\n",
        "\n",
        "# Configure the scoring environment\n",
        "aci_inference_config = InferenceConfig(runtime= \"python\",\n",
        "                                   source_directory = folder_name,\n",
        "                                   entry_script=\"scoring_script.py\",\n",
        "                                   conda_file=\"flight_delays_env.yml\")\n",
        "\n",
        "aci_deployment_config = AciWebservice.deploy_configuration(\n",
        "    cpu_cores = 1,\n",
        "    memory_gb = 1,\n",
        "    auth_enabled=True,\n",
        "    tags = {'name': aci_service_name},\n",
        "    description = 'Flights delay predictor ACI Web Service'\n",
        "    )\n",
        "\n",
        "try:\n",
        "    aci_service = Model.deploy(workspace=ws,\n",
        "                           name=aci_service_name,\n",
        "                           models=[model],\n",
        "                           inference_config=aci_inference_config,\n",
        "                           deployment_config=aci_deployment_config)\n",
        "\n",
        "    aci_service.wait_for_deployment(show_output=True)\n",
        "except WebserviceException as e:\n",
        "    print(f\"Service {aci_service_name} with the same name already exists. Use it\")\n",
        "    aci_service = Webservice(workspace=ws, name=aci_service_name)\n",
        "\n",
        "service = aci_service"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge 5: Consume the Real-time Endpoint\n",
        "\n",
        "Let's determine the URL to which these applications must submit their requests as well as the keys:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "endpoint = service.scoring_uri\n",
        "primary_key, secondary_key = service.get_keys()\n",
        "print(f'primary_key: {primary_key} \\nsecondary_key: {secondary_key} \\nEndpointUrl: {endpoint}')\n",
        "print(f\"compute type: {service.compute_type}\")"
      ]
    },
    {
      "source": [
        "Now that you know the endpoint URI, an application can simply make an HTTP request, sending the flight data in JSON (or binary) format, and receive back the predicted class(es)."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "\n",
        "x_new = [[4, 19, 5, 4, 18, 36, 837, -3.0, 1138],\n",
        "         [3, 76, 6, 4, 19, 36, 837, 98.0, 1234]]\n",
        "\n",
        "# Convert the array to a serializable list in a JSON document\n",
        "input_json = json.dumps({\"data\": x_new})\n",
        "\n",
        "# Set the content type\n",
        "request_headers = {\"Content-Type\": \"application/json\",\n",
        "                   \"Authorization\": \"Bearer \" + primary_key\n",
        "                   }\n",
        "\n",
        "predictions = requests.post(endpoint, input_json, headers=request_headers)\n",
        "\n",
        "predicted_classes = json.loads(predictions.json())\n",
        "\n",
        "for i in range(len(x_new)):\n",
        "    print(\"Flight {}\".format(x_new[i]), predicted_classes[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.7 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
        }
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.8.7-final",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}